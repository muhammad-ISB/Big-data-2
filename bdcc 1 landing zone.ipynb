{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0be9a13a-8f20-488b-af14-0f9cfb907a13",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Staging Zone\n",
    "Store the datasets in staging zone using appropriate schema, formats and partitions as considered appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e251b8d4-3b42-4dba-80f9-5fd3a6b339ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=3215667886385722#setting/sparkui/1231-130454-gdh4cens/driver-4414931548871816910\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fcddd78a110>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85eb07ea-58f7-4cbe-a56a-05e06b18e929",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa0dc1eb-31be-453f-aae7-a672ace5be9b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/FileStore/tables/customers.json</td><td>customers.json</td><td>15815082</td><td>1704000459000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/FileStore/tables/customers.json",
         "customers.json",
         15815082,
         1704000459000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs ls /FileStore/tables/customers.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d529d84e-ace4-4978-88b3-af7899f32c31",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[City: string, County: string, Customer Since: string, E Mail: string, Gender: string, Place Name: string, Region: string, State: string, Zip: bigint, age: double, cust_id: double, full_name: string]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custj =  spark.read.json(\"/FileStore/tables/customers.json\")\n",
    "custj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d18a2d8-b10f-4019-b07a-e9615311785a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[order_id: string, order_date: date, status: string, item_id: double, qty_ordered: double, price: double, value: double, discount_amount: double, total: double, category: string, payment_method: string, cust_id: double, year: int, month: string]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orderc = spark.read.csv(\"/FileStore/tables/orders.csv\", header=True, inferSchema=True)\n",
    "orderc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e715152-d480-4965-bd56-ea9dfe675a70",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "joined_df = custj.join(orderc, 'cust_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f47f35f1-c517-4335-81b6-349f863964cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------+--------------+--------------------+------+----------+------+-----+-----+----+-----------+---------+----------+--------------+--------+-----------+-----+------+-----------------+---------+-----------------+--------------+----+--------+\n|cust_id|  City|  County|Customer Since|              E Mail|Gender|Place Name|Region|State|  Zip| age|  full_name| order_id|order_date|        status| item_id|qty_ordered|price| value|  discount_amount|    total|         category|payment_method|year|   month|\n+-------+------+--------+--------------+--------------------+------+----------+------+-----+-----+----+-----------+---------+----------+--------------+--------+-----------+-----+------+-----------------+---------+-----------------+--------------+----+--------+\n|60124.0|Vinson|  Harmon|     8/22/2006|jani.titus@gmail.com|     F|    Vinson| South|   OK|73571|43.0|Titus, Jani|100354678|2020-10-01|      received|574772.0|       21.0| 89.9|1798.0|              0.0|   1798.0|    Men's Fashion|           cod|2020|Oct-2020|\n|60124.0|Vinson|  Harmon|     8/22/2006|jani.titus@gmail.com|     F|    Vinson| South|   OK|73571|43.0|Titus, Jani|100354678|2020-10-01|      received|574774.0|       11.0| 19.0| 190.0|              0.0|    190.0|    Men's Fashion|           cod|2020|Oct-2020|\n|60124.0|Vinson|  Harmon|     8/22/2006|jani.titus@gmail.com|     F|    Vinson| South|   OK|73571|43.0|Titus, Jani|100354680|2020-10-01|      complete|574777.0|        9.0|149.9|1199.2|              0.0|   1199.2|    Men's Fashion|           cod|2020|Oct-2020|\n|60124.0|Vinson|  Harmon|     8/22/2006|jani.titus@gmail.com|     F|    Vinson| South|   OK|73571|43.0|Titus, Jani|100354680|2020-10-01|      complete|574779.0|        9.0| 79.9| 639.2|              0.0|    639.2|    Men's Fashion|           cod|2020|Oct-2020|\n|60124.0|Vinson|  Harmon|     8/22/2006|jani.titus@gmail.com|     F|    Vinson| South|   OK|73571|43.0|Titus, Jani|100367357|2020-11-13|      received|595185.0|        2.0| 99.9|  99.9|              0.0|     99.9|    Men's Fashion|           cod|2020|Nov-2020|\n|60124.0|Vinson|  Harmon|     8/22/2006|jani.titus@gmail.com|     F|    Vinson| South|   OK|73571|43.0|Titus, Jani|100367357|2020-11-13|      received|595186.0|        2.0| 39.9|  39.9|              0.0|     39.9|    Men's Fashion|           cod|2020|Nov-2020|\n|60124.0|Vinson|  Harmon|     8/22/2006|jani.titus@gmail.com|     F|    Vinson| South|   OK|73571|43.0|Titus, Jani|100367360|2020-11-13|order_refunded|595192.0|        2.0| 47.6|  47.6|              0.0|     47.6|Mobiles & Tablets|           cod|2020|Nov-2020|\n|42485.0|Graham|Bradford|      2/4/1981| lee.eaker@gmail.com|     M|    Graham| South|   FL|32042|28.0| Eaker, Lee|100354677|2020-10-01|      canceled|574769.0|        2.0| 49.0|  49.0|              0.0|     49.0|Mobiles & Tablets|       Payaxis|2020|Oct-2020|\n|42485.0|Graham|Bradford|      2/4/1981| lee.eaker@gmail.com|     M|    Graham| South|   FL|32042|28.0| Eaker, Lee|100354677|2020-10-01|      canceled|574770.0|        2.0|135.0| 135.0|              0.0|    135.0|        Computing|       Payaxis|2020|Oct-2020|\n|42485.0|Graham|Bradford|      2/4/1981| lee.eaker@gmail.com|     M|    Graham| South|   FL|32042|28.0| Eaker, Lee|100354677|2020-10-01|      canceled|574771.0|        2.0|549.9| 549.9|              0.0|    549.9|       Appliances|       Payaxis|2020|Oct-2020|\n|42485.0|Graham|Bradford|      2/4/1981| lee.eaker@gmail.com|     M|    Graham| South|   FL|32042|28.0| Eaker, Lee|100356116|2020-10-08|order_refunded|577467.0|        1.0|549.9|   0.0|              0.0|      0.0|       Appliances|       Payaxis|2020|Oct-2020|\n|42485.0|Graham|Bradford|      2/4/1981| lee.eaker@gmail.com|     M|    Graham| South|   FL|32042|28.0| Eaker, Lee|100358724|2020-10-21|order_refunded|581862.0|        1.0|549.9|   0.0|              0.0|      0.0|       Appliances|           cod|2020|Oct-2020|\n|42485.0|Graham|Bradford|      2/4/1981| lee.eaker@gmail.com|     M|    Graham| South|   FL|32042|28.0| Eaker, Lee|100403034|2020-12-24|      canceled|656937.0|        2.0|254.8| 254.8|         39.80628|214.99372|       Appliances|       Easypay|2020|Dec-2020|\n|42485.0|Graham|Bradford|      2/4/1981| lee.eaker@gmail.com|     M|    Graham| South|   FL|32042|28.0| Eaker, Lee|100403034|2020-12-24|      canceled|656938.0|        2.0|315.5| 315.5|49.28916999999999|266.21083|       Appliances|       Easypay|2020|Dec-2020|\n|42485.0|Graham|Bradford|      2/4/1981| lee.eaker@gmail.com|     M|    Graham| South|   FL|32042|28.0| Eaker, Lee|100403034|2020-12-24|      canceled|656939.0|        2.0| 69.8|  69.8|         10.90455| 58.89545|    Home & Living|       Easypay|2020|Dec-2020|\n|42485.0|Graham|Bradford|      2/4/1981| lee.eaker@gmail.com|     M|    Graham| South|   FL|32042|28.0| Eaker, Lee|100403077|2020-12-24|      canceled|657023.0|        2.0|254.8| 254.8|         39.80628|214.99372|       Appliances|       Easypay|2020|Dec-2020|\n|42485.0|Graham|Bradford|      2/4/1981| lee.eaker@gmail.com|     M|    Graham| South|   FL|32042|28.0| Eaker, Lee|100403077|2020-12-24|      canceled|657024.0|        2.0|315.5| 315.5|49.28916999999999|266.21083|       Appliances|       Easypay|2020|Dec-2020|\n|42485.0|Graham|Bradford|      2/4/1981| lee.eaker@gmail.com|     M|    Graham| South|   FL|32042|28.0| Eaker, Lee|100403077|2020-12-24|      canceled|657025.0|        2.0| 69.8|  69.8|         10.90455| 58.89545|    Home & Living|       Easypay|2020|Dec-2020|\n|42485.0|Graham|Bradford|      2/4/1981| lee.eaker@gmail.com|     M|    Graham| South|   FL|32042|28.0| Eaker, Lee|100403411|2020-12-24|      canceled|657547.0|        2.0|254.8| 254.8|         39.80628|214.99372|       Appliances|       Easypay|2020|Dec-2020|\n|42485.0|Graham|Bradford|      2/4/1981| lee.eaker@gmail.com|     M|    Graham| South|   FL|32042|28.0| Eaker, Lee|100403411|2020-12-24|      canceled|657548.0|        2.0|315.5| 315.5|49.28916999999999|266.21083|       Appliances|       Easypay|2020|Dec-2020|\n+-------+------+--------+--------------+--------------------+------+----------+------+-----+-----+----+-----------+---------+----------+--------------+--------+-----------+-----+------+-----------------+---------+-----------------+--------------+----+--------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "joined_df.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "150d98d7-19d0-4073-9772-89bd8f4c710a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, StringType, LongType, DateType, IntegerType\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d10e1b26-f063-447e-8cd8-9420decca211",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# defining below the schema and assigning it to the dataset\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"cust_id\", DoubleType(), True),\n",
    "    StructField(\"City\", StringType(), True),\n",
    "    StructField(\"County\", StringType(), True),\n",
    "    StructField(\"Customer Since\", StringType(), True),\n",
    "    StructField(\"E Mail\", StringType(), True),\n",
    "    StructField(\"Gender\", StringType(), True),\n",
    "    StructField(\"Place Name\", StringType(), True),\n",
    "    StructField(\"Region\", StringType(), True),\n",
    "    StructField(\"State\", StringType(), True),\n",
    "    StructField(\"Zip\", LongType(), True),\n",
    "    StructField(\"age\", DoubleType(), True),\n",
    "    StructField(\"full_name\", StringType(), True),\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"order_date\", DateType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"item_id\", DoubleType(), True),\n",
    "    StructField(\"qty_ordered\", DoubleType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"value\", DoubleType(), True),\n",
    "    StructField(\"discount_amount\", DoubleType(), True),\n",
    "    StructField(\"total\", DoubleType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"payment_method\", StringType(), True),\n",
    "    StructField(\"year\", IntegerType(), True),\n",
    "    StructField(\"month\", StringType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd64b80e-dbdb-46dd-a08a-c81746a5e155",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Apply the schema to joined_df\n",
    "joined_df = joined_df.select([col(c).cast(schema[c].dataType) for c in schema.fieldNames()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eea222c4-6464-4afa-b970-3b1151ccd76f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ingest the DataFrame into the landing zone as Parquet\n",
    "landing_zone_path = \"/path/to/landing/zone\"\n",
    "joined_df.write.parquet(landing_zone_path, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eac0e28a-4ba0-40b9-a7f0-f1e96b4c7f4e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1899057920654282>, line 4\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[43mjoined_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpartitionBy\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtransaction_date\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcategory\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mRegion\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[43m               \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mparquet\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n",
       "\u001B[1;32m      3\u001B[0m \u001B[43m               \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43moverwrite\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n",
       "\u001B[0;32m----> 4\u001B[0m \u001B[43m               \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/path/to/optimized/dataset\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1463\u001B[0m, in \u001B[0;36mDataFrameWriter.save\u001B[0;34m(self, path, format, mode, partitionBy, **options)\u001B[0m\n",
       "\u001B[1;32m   1461\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39msave()\n",
       "\u001B[1;32m   1462\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m-> 1463\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:194\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    190\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    191\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    192\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    193\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 194\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    195\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    196\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: Partition column `transaction_date` not found in schema struct<cust_id:double,City:string,County:string,Customer Since:string,E Mail:string,Gender:string,Place Name:string,Region:string,State:string,Zip:bigint,age:double,full_name:string,order_id:string,order_date:date,status:string,item_id:double,qty_ordered:double,price:double,value:double,discount_amount:double,total:double,category:string,payment_method:string,year:int,month:string>."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-1899057920654282>, line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[43mjoined_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpartitionBy\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtransaction_date\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcategory\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mRegion\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m               \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mparquet\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m               \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43moverwrite\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[0;32m----> 4\u001B[0m \u001B[43m               \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/path/to/optimized/dataset\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1463\u001B[0m, in \u001B[0;36mDataFrameWriter.save\u001B[0;34m(self, path, format, mode, partitionBy, **options)\u001B[0m\n\u001B[1;32m   1461\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39msave()\n\u001B[1;32m   1462\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1463\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:194\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    190\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    191\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    192\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    193\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 194\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    195\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    196\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: Partition column `transaction_date` not found in schema struct<cust_id:double,City:string,County:string,Customer Since:string,E Mail:string,Gender:string,Place Name:string,Region:string,State:string,Zip:bigint,age:double,full_name:string,order_id:string,order_date:date,status:string,item_id:double,qty_ordered:double,price:double,value:double,discount_amount:double,total:double,category:string,payment_method:string,year:int,month:string>.",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: Partition column `transaction_date` not found in schema struct<cust_id:double,City:string,County:string,Customer Since:string,E Mail:string,Gender:string,Place Name:string,Region:string,State:string,Zip:bigint,age:double,full_name:string,order_id:string,order_date:date,status:string,item_id:double,qty_ordered:double,price:double,value:double,discount_amount:double,total:double,category:string,payment_method:string,year:int,month:string>.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f73b7c8b-5d7b-4b7c-90ec-98531cbb3eb4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3854757434761437,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bdcc 1 landing zone",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
